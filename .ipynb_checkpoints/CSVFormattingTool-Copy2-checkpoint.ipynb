{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latest update\n",
    "# April 26, 2017\n",
    "# Changes done by: Jimmy Vivas\n",
    "# Added code description\n",
    "\n",
    "# This code was developed by Jimmy Vivas\n",
    "\n",
    "# The code will take all CSv files from a specific directory and process them at the same Time\n",
    "# The code will get replace strings in the values for numeric data\n",
    "#   It will assign an integer value for each distinct string for each column (sensor)\n",
    "#   The integer value will be reset it for each sensor\n",
    "#   The code does not evaluare wether or not the sensor should stay. THe analyst will make this decision\n",
    "#   Using Previse or any other tool\n",
    "# The code will split the csv file if the result is more than 1MM rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mac Path\n",
    "#CSVPath1 =  '/Users/jvivas/Documents/XMT Baytwon Sensor Data' \\\n",
    "#            '/Sensor Data/Testing'\n",
    "\n",
    "CSVPath1 = '/Users/jvivas/Documents/Aspen/PTT GC PoC/Sensor Data/A210 - Quench Oil Tower/To Be Processed'\n",
    "\n",
    "# Windows Path\n",
    "# CSVPath1 = 'C:/Users/jvivas/Dropbox/Mtell Customer Projects/XOM BayTown RHC MEA Tower Foaming/Sensor Data/' \\\n",
    "#           'To be Processed'\n",
    "\n",
    "# CSVPath1 = 'C:/Mtell/Projects/XOM Baytown POC/Sensor Data/ToBeFormatted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CSVFileList = []\n",
    "NanPHDTagList = []\n",
    "CSVFileListAll = listdir(CSVPath1)\n",
    "\n",
    "m = len(CSVFileListAll)\n",
    "\n",
    "for i in range(m):\n",
    "    fileNameStr = CSVFileListAll[i]\n",
    "    fileStr = fileNameStr.split('.')[0]\n",
    "    fileExt = fileNameStr.split('.')[1]\n",
    "    if fileExt == \"csv\":\n",
    "        CSVFileList.append(fileNameStr)\n",
    "\n",
    "n = len(CSVFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncomment following cell for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetFileSize(file):\n",
    "    import os\n",
    "    csvFileStatInfo = os.stat(file)\n",
    "    csvFileSizeGB = csvFileStatInfo.st_size/1000000000\n",
    "    return(csvFileSizeGB)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RenameColumn(df2):\n",
    "# Ranane Date and time column to DATETIME\n",
    "    new_cols = ['DATETIME']\n",
    "    df2.rename(columns=dict(zip(df2.columns[[0]], new_cols)),inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyDateFormat(df2):\n",
    "\n",
    "    # duplicate df2 to apply date and time format \n",
    "    df2 = df2.copy()    \n",
    "    # Change new datetime column to datetime format\n",
    "    df2['DATETIME'] = pd.to_datetime(df2['DATETIME'])    \n",
    "    # Change datetime column format to look like 01/31/2015 0:00:00\n",
    "    df2['DATETIME'] = df2['DATETIME'].dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SplitCSVFile_GetStrings():\n",
    "    chunksize = 25000\n",
    "\n",
    "    i = 0\n",
    "    j = 1\n",
    "\n",
    "    for df in pd.read_csv(csvfile, chunksize=chunksize, iterator=True, low_memory=False):\n",
    "        # df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) \n",
    "        df.index += j\n",
    "        i+=1\n",
    "        j = df.index[-1] + 1\n",
    "        h = 0\n",
    "\n",
    "        # print(\"Loading Segment: %s\" % CSVFileList[i])\n",
    "\n",
    "        df = df.copy()\n",
    "        # Change new datetime column to datetime format\n",
    "        df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "        # In[53]:\n",
    "        # Change datetime column format to look like 01/31/2015 0:00:00\n",
    "        df['DATETIME'] = df['DATETIME'].dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "        # Code to do Transposing\n",
    "        # Create two dataframes df1 only with tags and descriptions. df2 tag with values\n",
    "        if i == 1:\n",
    "            df1 = df[0:1]  # FIRST ROW\n",
    "            # Indexing dataframe df1\n",
    "            df1 = df1.set_index('DATETIME')\n",
    "        else:\n",
    "            if i > 1:\n",
    "                df1 = df1\n",
    "                \n",
    "        df2 = df[1:len(df)]  # SECOND TO LAST ROW\n",
    "\n",
    "        # Indexing dataframe df2\n",
    "        df2 = df2.set_index('DATETIME')\n",
    "        \n",
    "        # Export CSV chunk for each loop\n",
    "        df2.to_csv(csvfile + 'chunk ' + str(i), index=False)\n",
    "        \n",
    "        SensorStringListForEachChunk = ExtractStrings(df2)\n",
    "    SensorStringListAllChunks = list(set(SensorStringListForEachChunk))\n",
    "    return(df2, SensorStringListAllChunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExtractStrings(df2):\n",
    "    print ('Converting df to numeric')\n",
    "    # Convert columns to numbers those that has string wil be converted to numpy null = NaN\n",
    "    df2_with_nan = df2.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    print (df2.head())\n",
    "    print (df2_with_nan.head())\n",
    "    # Ge the list of sensors with Strings\n",
    "    df3 = pd.DataFrame(df2_with_nan.isnull().any(axis=0))\n",
    "    df3 = df3.reset_index()\n",
    "    SensorsWithStrings = df3[df3[0]==True]['index'].tolist()\n",
    "    \n",
    "    # Replace nan with 'null' and create new df\n",
    "    df2_with_null = df2_with_nan.fillna(value='null')\n",
    "    \n",
    "    # Here we filter the columns that are objects and create a dataframe with those columns\n",
    "    colsObject = df2.select_dtypes(include=['object']).columns\n",
    "    TotalColumns = df2.columns\n",
    "    \n",
    "    # print status\n",
    "    if len(SensorsWithStrings) == 0:\n",
    "        print ('No strings in this dataframe')\n",
    "    else:\n",
    "        print (str(len(SensorsWithStrings)) + ' columns contain strings out of ' + str(len(TotalColumns)) + ' columns')\n",
    "        if (len(SensorsWithStrings)/len(TotalColumns)) > 0.5:\n",
    "            print ('this process will take some time depending of the size of the file and the PC resources')\n",
    "\n",
    "        # Variable initialization\n",
    "        j = 0\n",
    "        StringListForAllSensors = []\n",
    "\n",
    "        # Create dictionary from dataframe columns (sensors) that have strings only\n",
    "        SensorDictionary = {}.fromkeys(SensorsWithStrings, [])\n",
    "        \n",
    "        # Loop to go thru each column and convert the characters string to numbers    \n",
    "        for j in range(len(SensorsWithStrings)):\n",
    "            # from IPython.core.debugger import Tracer\n",
    "            # Tracer()() #this one triggers the debugger\n",
    "\n",
    "            # iterate thru each column in the dataframe\n",
    "            # for j in range(len(list(SensorDictionary))):\n",
    "            # update sensor name for each column\n",
    "            Sensor = list(SensorDictionary)[j]\n",
    "            # javc8581\n",
    "            print ('Obtaining Strings for Sensor: ' + Sensor)\n",
    "            # Clear List of String for each Sensor\n",
    "            SensorStringResult = []\n",
    "            # Iterate thru each row value\n",
    "\n",
    "            # Get rows that have null in the actual sensor column\n",
    "            # df2_with_null = df2_with_null[df2_with_null['tagname 1']=='null']['tagname 1']\n",
    "            result = df2_with_nan[df2_with_nan[Sensor].isnull()][Sensor]\n",
    "\n",
    "            # Convert list of nulls to dataframe and reset the datetime index\n",
    "            result_df = pd.DataFrame(result)\n",
    "            result_df = result_df.reset_index()\n",
    "             \n",
    "            print (result_df.head())\n",
    "            print (df2.head())\n",
    "            # debugging\n",
    "            # print (result_df)\n",
    "            # debugging\n",
    "\n",
    "            # Getting list of Strings for Sensor\n",
    "            SensorStringResult = df2[df2['DATETIME'].isin(result_df.loc[:,\"DATETIME\"].values.tolist())][Sensor]\n",
    "\n",
    "            # Adding Strings to Sensor in Sensor Dictionary\n",
    "            SensorDictionary[Sensor] = list(set(SensorStringResult))\n",
    "\n",
    "            # Debugging\n",
    "            # print (list(set(SensorStringResult)))\n",
    "            # Debugging\n",
    "\n",
    "            # Adding String for Sensor to General String List\n",
    "            StringListForAllSensors.extend(SensorStringResult)\n",
    "            \n",
    "            print ('Chunnk: ' + str(i) + ' Processed')\n",
    "\n",
    "            print ('These are all the strings found in the chunk: ' + str(i) + str(StringListForAllSensors))\n",
    "    return (StringListForAllSensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadCSV(FileAndPath, file):\n",
    "    print ('Loading ' + file)\n",
    "    df2 = pd.read_csv(FileAndPath, low_memory=False)\n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading A210_1Column.csv\n",
      "Ranming date and time column\n",
      "Applying date and timeformat\n",
      "Extracting strings from csv file\n",
      "Converting df to numeric\n",
      "      DATETIME         I4-TI2104.PV\n",
      "0          NaN  A-210 QO TOWER BTMS\n",
      "1       1/1/14         158.78685000\n",
      "2  1/1/14 0:05         158.68770000\n",
      "3  1/1/14 0:10         158.70590000\n",
      "4  1/1/14 0:15         158.96510000\n",
      "   DATETIME  I4-TI2104.PV\n",
      "0       NaN           NaN\n",
      "1       NaN     158.78685\n",
      "2       NaN     158.68770\n",
      "3       NaN     158.70590\n",
      "4       NaN     158.96510\n",
      "2 columns contain strings out of 2 columns\n",
      "this process will take some time depending of the size of the file and the PC resources\n",
      "Obtaining Strings for Sensor: I4-TI2104.PV\n",
      "   index  I4-TI2104.PV\n",
      "0      0           NaN\n",
      "1  16600           NaN\n",
      "2  22588           NaN\n",
      "3  22589           NaN\n",
      "4  22592           NaN\n",
      "      DATETIME         I4-TI2104.PV\n",
      "0          NaN  A-210 QO TOWER BTMS\n",
      "1       1/1/14         158.78685000\n",
      "2  1/1/14 0:05         158.68770000\n",
      "3  1/1/14 0:10         158.70590000\n",
      "4  1/1/14 0:15         158.96510000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'the label [DATETIME] is not in the [columns]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                     \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1389\u001b[0m                 raise KeyError(\"the label [%s] is not in the [%s]\" %\n\u001b[0;32m-> 1390\u001b[0;31m                                (key, self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the label [DATETIME] is not in the [columns]'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-62ee0dc0ba8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Extract strings from dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting strings from csv file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mStringListForAllSensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtractStrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'These are all the strings found in the csv file: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringListForAllSensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-93d28e45cf4c>\u001b[0m in \u001b[0;36mExtractStrings\u001b[0;34m(df2)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Getting list of Strings for Sensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mSensorStringResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATETIME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"DATETIME\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Adding Strings to Sensor in Sensor Dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0;31m# we have yielded a scalar ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m                 \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jvivas/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1388\u001b[0m                                     \"key\")\n\u001b[1;32m   1389\u001b[0m                 raise KeyError(\"the label [%s] is not in the [%s]\" %\n\u001b[0;32m-> 1390\u001b[0;31m                                (key, self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the label [DATETIME] is not in the [columns]'"
     ]
    }
   ],
   "source": [
    "csvfile = CSVPath1 + \"/\" + CSVFileList[0]\n",
    "csvFileSizeGB = GetFileSize(csvfile)\n",
    "if csvFileSizeGB > 1:\n",
    "    SplitCSVFile_GetStrings()\n",
    "else:\n",
    "    \n",
    "    df2 = LoadCSV(csvfile, CSVFileList[0])\n",
    "    \n",
    "    # Rename date and time column\n",
    "    print ('Ranming date and time column')\n",
    "    RenameColumn(df2)\n",
    "    # Apply Date and time format to dataframe\n",
    "    print ('Applying date and timeformat')\n",
    "    ApplyDateFormat(df2)\n",
    "    # Extract strings from dataframe\n",
    "    print ('Extracting strings from csv file')\n",
    "    StringListForAllSensors = ExtractStrings(df2)\n",
    "\n",
    "print ('These are all the strings found in the csv file: ' + str(StringListForAllSensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReplaceStrings(df):\n",
    "    print(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPreviseFormat(df):\n",
    "    print(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExportPreviseCSVFile(df):\n",
    "    print(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping DATETIME index to merge df1 and df2\n",
    "    if i == 1:\n",
    "        df1 = df1.reset_index(drop=False)\n",
    "    df2 = df2.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize = 25000\n",
    "csvfile = CSVPath1 + \"/\" + CSVFileList[0]\n",
    "i = 0\n",
    "j = 1\n",
    "print ('Loading ' + CSVFileList[0] + ' file')\n",
    "\n",
    "for df in pd.read_csv(csvfile, chunksize=chunksize, iterator=True, low_memory=False):\n",
    "    # df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) \n",
    "    df.index += j\n",
    "    i+=1\n",
    "    j = df.index[-1] + 1\n",
    "    # print ('exporting file' + str(i))\n",
    "    # df.to_csv(CSVPath1 + \"/\" + 'test' + str(i) + '.csv')\n",
    "    h = 0\n",
    "\n",
    "    # print(\"Loading Segment: %s\" % CSVFileList[i])\n",
    "\n",
    "    # In[43]:\n",
    "    # %time df = pd.read_csv(csvfile, index_col=False, sep=',', low_memory=False)\n",
    "    \n",
    "    # Ranane Date and time column to DATETIME\n",
    "    new_cols = ['DATETIME']\n",
    "    df.rename(columns=dict(zip(df.columns[[0]], new_cols)),inplace=True)\n",
    "    \n",
    "    # In[51]:\n",
    "    # Need to do this to avoid \n",
    "    df = df.copy()\n",
    "    # Change new datetime column to datetime format\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "    # In[53]:\n",
    "    # Change datetime column format to look like 01/31/2015 0:00:00\n",
    "    df['DATETIME'] = df['DATETIME'].dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    # Code to do Transposing\n",
    "    # Create two dataframes df1 only with tags and descriptions. df2 tag with values\n",
    "    if i == 1:\n",
    "        df1 = df[0:1]  # FIRST ROW\n",
    "        # Indexing dataframe df1\n",
    "        df1 = df1.set_index('DATETIME')\n",
    "    else:\n",
    "        if i > 1:\n",
    "            df1 = df1\n",
    "    \n",
    "    df2 = df[1:len(df)]  # SECOND TO LAST ROW\n",
    "    \n",
    "    # Indexing dataframe df2\n",
    "    df2 = df2.set_index('DATETIME')\n",
    "    \n",
    "    print ('Converting df to numeric')\n",
    "    # Convert columns to numbers those that has string wil be converted to numpy null = NaN\n",
    "    %time df2_with_nan = df2.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "\n",
    "    # Ge the list of sensors with Strings\n",
    "    df3 = pd.DataFrame(df2_with_nan.isnull().any(axis=0))\n",
    "    df3 = df3.reset_index()\n",
    "    SensorsWithStrings = df3[df3[0]==True]['index'].tolist()\n",
    "    \n",
    "    # Replace nan with 'null' and create new df\n",
    "    df2_with_null = df2_with_nan.fillna(value='null')\n",
    "    \n",
    "    # Here we filter the columns that are objects and create a dataframe with those columns\n",
    "    colsObject = df2.select_dtypes(include=['object']).columns\n",
    "    TotalColumns = df2.columns\n",
    "    \n",
    "    # print status\n",
    "    if len(SensorsWithStrings) == 0:\n",
    "        print ('No strings in this dataframe')\n",
    "    else:\n",
    "        print (str(len(SensorsWithStrings)) + ' columns contain strings out of ' + str(len(TotalColumns)) + ' columns')\n",
    "        if (len(SensorsWithStrings)/len(TotalColumns)) > 0.5:\n",
    "            print ('this process will take some time depending of the size of the file and the PC resources')\n",
    "\n",
    "        # Dropping DATETIME index to merge df1 and df2\n",
    "        if i == 1:\n",
    "            df1 = df1.reset_index(drop=False)\n",
    "\n",
    "        df2 = df2.reset_index(drop=False)\n",
    "\n",
    "        # Variable initialization\n",
    "        j = 0\n",
    "        StringListForAllSensors = []\n",
    "\n",
    "        # Create dictionary from dataframe columns (sensors) that have strings only\n",
    "        SensorDictionary = {}.fromkeys(SensorsWithStrings, [])\n",
    "\n",
    "        # Loop to go thru each column and convert the characters string to numbers    \n",
    "\n",
    "        for j in range(len(SensorsWithStrings)):\n",
    "            # from IPython.core.debugger import Tracer\n",
    "            # Tracer()() #this one triggers the debugger\n",
    "\n",
    "            # iterate thru each column in the dataframe\n",
    "            # for j in range(len(list(SensorDictionary))):\n",
    "            # update sensor name for each column\n",
    "            Sensor = list(SensorDictionary)[j]\n",
    "            # javc8581\n",
    "            print ('Obtaining Strings for Sensor: ' + Sensor)\n",
    "            # Clear List of String for each Sensor\n",
    "            SensorStringResult = []\n",
    "            # Iterate thru each row value\n",
    "\n",
    "            # Get rows that have null in the actual sensor column\n",
    "            # df2_with_null = df2_with_null[df2_with_null['tagname 1']=='null']['tagname 1']\n",
    "            result = df2_with_nan[df2_with_nan[Sensor].isnull()][Sensor]\n",
    "\n",
    "            # Convert list of nulls to dataframe and reset the datetime index\n",
    "            result_df = pd.DataFrame(result)\n",
    "            result_df = result_df.reset_index()\n",
    "\n",
    "            print (result_df.head())\n",
    "            print (df2.head())\n",
    "            \n",
    "            # debugging\n",
    "            # print (result_df)\n",
    "            # debugging\n",
    "\n",
    "            # Getting list of Strings for Sensor\n",
    "            SensorStringResult = df2[df2['DATETIME'].isin(result_df.loc[:,\"DATETIME\"].values.tolist())][Sensor]\n",
    "\n",
    "            # Adding Strings to Sensor in Sensor Dictionary\n",
    "            SensorDictionary[Sensor] = list(set(SensorStringResult))\n",
    "\n",
    "            # Debugging\n",
    "            # print (list(set(SensorStringResult)))\n",
    "            # Debugging\n",
    "\n",
    "            # Adding String for Sensor to General String List\n",
    "            StringListForAllSensors.extend(SensorStringResult)\n",
    "            \n",
    "            print ('Chunnk: ' + str(i) + ' Processed')\n",
    "\n",
    "        # print (SensorDictionary)\n",
    "        # Removing Duplicate Strings\n",
    "        StringListForAllSensors = list(set(StringListForAllSensors))\n",
    "        print ('These are all the strings found in the csv file: ' + str(StringListForAllSensors))\n",
    "        \n",
    "        # Dropping DATETime index to merge df1 and df2\n",
    "        # df1 = df1.reset_index(drop=False)\n",
    "        # df2 = df2.reset_index(drop=False)\n",
    "\n",
    "        # Converting Historian files to VTQ format (DATETime, TAGNAME, DESCRIPTION, VALUE)\n",
    "        mdf = pd.merge(pd.melt(df1, id_vars=['DATETIME'], var_name='TAGNAME',\n",
    "                               value_name='DESCRIPTION')[['TAGNAME', 'DESCRIPTION']],\n",
    "                       pd.melt(df2, id_vars=['DATETIME'], var_name='TAGNAME',\n",
    "                               value_name='VALUE'),\n",
    "                       on=['TAGNAME'])\n",
    "\n",
    "        # Sort columns by VTQ format\n",
    "        mdf = mdf[['DATETIME', 'TAGNAME', 'DESCRIPTION', 'VALUE']]\n",
    "        \n",
    "        # Exporting PHD Tag CSV file\n",
    "        m = 1\n",
    "        rows = 1000000\n",
    "        totalRows = len(mdf)\n",
    "        loops = math.ceil(totalRows/rows) + 1\n",
    "\n",
    "        for m in range(loops): #need to round this\n",
    "            a = (rows*m) - rows\n",
    "            if totalRows <= rows:\n",
    "                b = totalRows\n",
    "                print('Exporting ' + str(CSVFileList[0].replace('.csv', '')) + ' Historian File')\n",
    "                print(\"\")\n",
    "                mdf[a:b].to_csv(CSVPath1 + '/' + str(CSVFileList[0].replace('.csv', '')) + '_Formatted.csv', index=False)\n",
    "            else:\n",
    "                if (rows*m) >= totalRows:\n",
    "                    b = totalRows\n",
    "                    print('Exporting ' + str(CSVFileList[0].replace('.csv', '')) + ' chunk' + str(m) + ' Historian File')\n",
    "                    print(\"\")\n",
    "                    mdf[a:b].to_csv(CSVPath1 + '/' + str(CSVFileList[0].replace('.csv', '')) + '_Formatted_chunk' + str(m) + '.csv', index=False)\n",
    "                else:\n",
    "                    b = (rows*m) - 1\n",
    "                    print('Exporting ' + str(CSVFileList[0].replace('.csv', '')) + ' chunk' + str(m) + ' Historian File')\n",
    "                    print(\"\")\n",
    "                    mdf[a:b].to_csv(CSVPath1 + '/' + str(CSVFileList[0].replace('.csv', '')) + '_Formatted_chunk' + str(m) + '.csv', index=False)\n",
    "            # df2.to_csv(CSVPath1 + \"/\" + 'test' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(SensorDictionary)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_with_nan.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_with_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_with_null = df2_with_nan.fillna(value='null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_with_null.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = df2_with_nan[df2_with_nan['tagname 1'].isnull()]['tagname 1']\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df = result_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df.loc[:,\"DATETIME\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to query dataframe  with multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SensorStringResult = df2[df2['DATETIME'].isin(result_df.loc[:,\"DATETIME\"].values.tolist())]['tagname 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SensorStringList = SensorStringResult.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SensorStringList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df2_with_nan.isnull().any(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = df3.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df2_with_nan.isnull().any(axis=0))\n",
    "df3 = df3.reset_index()\n",
    "SensorsWithStrings = df3[df3[0]==True]['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SensorDictionary = {}.fromkeys(SensorsWithStrings, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SensorDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(SensorDictionary)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_with_nan[df2_with_nan.isnull().any(axis=1)].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df['DATETIME'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(df2[df2['DATETIME']==result_df['DATETIME'][0]]['tagname 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a['tagname 1'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now we have to replace string for specific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if pd.unique(df2[[colsObject[1]]].values.ravel()) != 'null':\n",
    "# Here we are assign a integer to each unique row value, the integer will increase\n",
    "# from 0 to x where x is the total unique values\n",
    "    # df2[[df2.columns.get_loc(colsObject[y])]] = pd.factorize(df2.iloc[:,df2.columns.get_loc(colsObject[y])])[0]\n",
    "\n",
    "# Replacing known test to null\n",
    "df2.replace({'test': 'null', 'hello there': 'null', 'wow more text': 'null'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we convert the data to linear format Previse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={'DATETIME': 'DATETime'})\n",
    "df2 = df2.rename(columns={'DATETIME': 'DATETime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropping DATETime index to merge df1 and df2\n",
    "df1 = df1.reset_index(drop=False)\n",
    "df2 = df2.reset_index(drop=False)\n",
    "\n",
    "# Converting Historian files to VTQ format (DATETime, TAGNAME, DESCRIPTION, VALUE)\n",
    "mdf = pd.merge(pd.melt(df1, id_vars=['DATETime'], var_name='TAGNAME',\n",
    "                       value_name='DESCRIPTION')[['TAGNAME', 'DESCRIPTION']],\n",
    "               pd.melt(df2, id_vars=['DATETime'], var_name='TAGNAME',\n",
    "                       value_name='VALUE'),\n",
    "               on=['TAGNAME'])\n",
    "\n",
    "# Sort columns by VTQ format\n",
    "mdf = mdf[['DATETime', 'TAGNAME', 'DESCRIPTION', 'VALUE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Here we split the CSV data by file and each file will contain 1 million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exporting PHD Tag CSV file\n",
    "j = 1\n",
    "rows = 1000000\n",
    "totalRows = len(mdf)\n",
    "loops = math.ceil(totalRows/rows) + 1\n",
    "\n",
    "for j in range(loops): #need to round this\n",
    "    a = (rows*j) - rows\n",
    "    if totalRows <= rows:\n",
    "        b = totalRows\n",
    "        print('Exporting ' + str(CSVFileList[i].replace('.csv', '')) + ' Historian File')\n",
    "        print(\"\")\n",
    "        mdf[a:b].to_csv(CSVPath1 + '/' + str(CSVFileList[i].replace('.csv', '')) + '_Formatted.csv', index=False)\n",
    "    else:\n",
    "        if (rows*j) >= totalRows:\n",
    "            b = totalRows\n",
    "            print('Exporting ' + str(CSVFileList[i].replace('.csv', '')) + ' chunk' + str(j) + ' Historian File')\n",
    "            print(\"\")\n",
    "            mdf[a:b].to_csv(CSVPath1 + '/' + str(CSVFileList[i].replace('.csv', '')) + '_Formatted_chunk' + str(j) + '.csv', index=False)\n",
    "        else:\n",
    "            b = (rows*j) - 1\n",
    "            print('Exporting ' + str(CSVFileList[i].replace('.csv', '')) + ' chunk' + str(j) + ' Historian File')\n",
    "            print(\"\")\n",
    "            mdf[a:b].to_csv(CSVPath1 + '/' + str(CSVFileList[i].replace('.csv', '')) + '_Formatted_chunk' + str(j) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
