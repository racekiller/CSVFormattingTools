{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latest update\n",
    "# April 26, 2017\n",
    "# Changes done by: Jimmy Vivas\n",
    "# Added code description\n",
    "\n",
    "# This code was developed by Jimmy Vivas\n",
    "\n",
    "# The code will take all CSv files from a specific directory and process them at the same Time\n",
    "# The code will get replace strings in the values for numeric data\n",
    "#   It will assign an integer value for each distinct string for each column (sensor)\n",
    "#   The integer value will be reset it for each sensor\n",
    "#   The code does not evaluare wether or not the sensor should stay. THe analyst will make this decision\n",
    "#   Using Previse or any other tool\n",
    "# The code will split the csv file if the result is more than 1MM rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mac Path\n",
    "#CSVPath1 =  '/Users/jvivas/Documents/XMT Baytwon Sensor Data' \\\n",
    "#            '/Sensor Data/Testing'\n",
    "\n",
    "# CSVPath1 = '/Users/jvivas/Documents/Aspen/GASCO/Sensor Data/To Be Processed'\n",
    "\n",
    "# Windows Path\n",
    "CSVPath1 = 'C:/Users/vivasj/Documents/Aspen/PTT GC/C002B/DATA/To be Processed'\n",
    "FinalPath = 'C:/Users/vivasj/Documents/Aspen/PTT GC/C002B/DATA/Processed'\n",
    "\n",
    "# CSVPath1 = 'C:/Mtell/Projects/XOM Baytown POC/Sensor Data/ToBeFormatted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSVFileList = []\n",
    "NanPHDTagList = []\n",
    "CSVFileListAll = listdir(CSVPath1)\n",
    "\n",
    "m = len(CSVFileListAll)\n",
    "\n",
    "for i in range(m):\n",
    "    fileNameStr = CSVFileListAll[i]\n",
    "    fileStr = fileNameStr.split('.')[0]\n",
    "    fileExt = fileNameStr.split('.')[1]\n",
    "    if fileExt == \"csv\":\n",
    "        CSVFileList.append(fileNameStr)\n",
    "\n",
    "n = len(CSVFileList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncomment following cell for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetFileSize(file):\n",
    "    import os\n",
    "    csvFileStatInfo = os.stat(file)\n",
    "    csvFileSizeGB = csvFileStatInfo.st_size/1000000000\n",
    "    return(csvFileSizeGB)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RenameColumn(df2):\n",
    "# Ranane Date and time column to DATETIME\n",
    "    new_cols = ['DATETIME']\n",
    "    df2.rename(columns=dict(zip(df2.columns[[0]], new_cols)),inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyDateFormat(df2):\n",
    "\n",
    "    # duplicate df2 to apply date and time format \n",
    "    df2 = df2.copy()    \n",
    "    # Change new datetime column to datetime format\n",
    "    df2['DATETIME'] = pd.to_datetime(df2['DATETIME'])    \n",
    "    # Change datetime column format to look like 01/31/2015 0:00:00\n",
    "    df2['DATETIME'] = df2['DATETIME'].dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SplitCSVFile_GetStrings(CSVFileWithPath, CSVFile):\n",
    "    chunksize = 25000\n",
    "\n",
    "    i = 0\n",
    "    j = 1\n",
    "    \n",
    "    SensorStringListAllChunks = []\n",
    "    StringListForAllSensors = []\n",
    "    StringListForEachChunk = []\n",
    "        \n",
    "    print ('Loading ' + CSVFile + ' file')\n",
    "    for df in pd.read_csv(CSVFileWithPath, chunksize=chunksize, iterator=True, low_memory=False):\n",
    "        # df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) \n",
    "        df.index += j\n",
    "        i+=1\n",
    "        j = df.index[-1] + 1\n",
    "        h = 0\n",
    "        \n",
    "        # Rename date and time column\n",
    "        print ('Renaming date and time column')\n",
    "        RenameColumn(df)\n",
    "\n",
    "        # Apply Date and time format to dataframe\n",
    "        print ('Applying date and time format')\n",
    "        ApplyDateFormat(df)\n",
    "\n",
    "        # Code to do Transposing\n",
    "        # Create two dataframes df1 only with tags and descriptions. df2 tag with values\n",
    "        if i == 1:\n",
    "            df1 = df[0:1]  # FIRST ROW\n",
    "            # Indexing dataframe df1\n",
    "            df1 = df1.set_index('DATETIME')\n",
    "        else:\n",
    "            if i > 1:\n",
    "                df1 = df1\n",
    "                \n",
    "        df2 = df[1:len(df)]  # SECOND TO LAST ROW\n",
    "\n",
    "        # Indexing dataframe df2\n",
    "        df2 = df2.set_index('DATETIME')\n",
    "        \n",
    "        # Export CSV chunk for each loop\n",
    "        print ('Exporting chunk' + str(i))\n",
    "        df.to_csv(CSVFileWithPath.replace('.csv', '') + '_chunk_ ' + str(i) + '.csv', index=True)\n",
    "        \n",
    "        print ('Getting the list for chunk' + str(i))\n",
    "        StringListForEachChunk = ExtractStrings(df1, df2)\n",
    "        \n",
    "        print ('These are all the strings found in the chunk ' + str(i) + ':' + str(StringListForEachChunk))\n",
    "        \n",
    "        SensorStringListAllChunks.extend(StringListForEachChunk)\n",
    "\n",
    "    SensorStringListAllChunks = list(set(SensorStringListAllChunks))\n",
    "    return(SensorStringListAllChunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExtractStrings(df2_1, df2_2):\n",
    "        \n",
    "    print ('Converting df to numeric')\n",
    "    # Convert columns to numbers those that has string wil be converted to numpy null = NaN\n",
    "    df2_with_nan = df2_2.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    # print (df2_2.head())\n",
    "    # print (df2_with_nan.head())\n",
    "    # Ge the list of sensors with Strings\n",
    "    df3 = pd.DataFrame(df2_with_nan.isnull().any(axis=0))\n",
    "    df3 = df3.reset_index()\n",
    "    SensorsWithStrings = df3[df3[0]==True]['index'].tolist()\n",
    "    \n",
    "    # Replace nan with 'null' and create new df\n",
    "    df2_with_null = df2_with_nan.fillna(value='null')\n",
    "    \n",
    "    # Here we filter the columns that are objects and create a dataframe with those columns\n",
    "    colsObject = df2_2.select_dtypes(include=['object']).columns\n",
    "    TotalColumns = df2_2.columns\n",
    "    \n",
    "    print ('Total columns with text: ' + str(TotalColumns))\n",
    "    print ('Sensors with text in their Values: ' + str(SensorsWithStrings))\n",
    "\n",
    "    AllSensorsStringList = []\n",
    "    \n",
    "    if len(SensorsWithStrings) == 0:\n",
    "        print ('No strings in this dataframe')\n",
    "    else:\n",
    "        # print (str(len(SensorsWithStrings)) + ' columns contain strings out of ' + str(len(TotalColumns)) + ' columns')\n",
    "        # if (len(SensorsWithStrings)/len(TotalColumns)) > 0.5:\n",
    "            # print ('this process will take some time depending of the size of the file and the PC resources')\n",
    "\n",
    "        # Dropping DATETIME index to merge df1 and df2\n",
    "        df2_1 = df2_1.reset_index(drop=False)\n",
    "        df2_2 = df2_2.reset_index(drop=False)\n",
    "        \n",
    "        # Variable initialization\n",
    "        j = 0\n",
    "\n",
    "        StringListForCurrentSensor = []\n",
    "\n",
    "        # Create dictionary from dataframe columns (sensors) that have strings only\n",
    "        SensorDictionary = {}.fromkeys(SensorsWithStrings, [])\n",
    "        \n",
    "        # Loop to go thru each column and convert the characters string to numbers    \n",
    "        for j in range(len(SensorsWithStrings)):\n",
    "            # from IPython.core.debugger import Tracer\n",
    "            # Tracer()() #this one triggers the debugger\n",
    "\n",
    "            # iterate thru each column in the dataframe\n",
    "            # for j in range(len(list(SensorDictionary))):\n",
    "            # update sensor name for each column\n",
    "            Sensor = list(SensorDictionary)[j]\n",
    "\n",
    "            print ('Processing Sensor: ' + Sensor)\n",
    "            # Clear List of String for each Sensor\n",
    "            SensorStringResult = []\n",
    "\n",
    "            # Get rows that have null in the actual sensor column\n",
    "            result = df2_with_nan[df2_with_nan[Sensor].isnull()][Sensor]\n",
    "\n",
    "            # Convert list of nulls to dataframe and reset the datetime index\n",
    "            result_df = pd.DataFrame(result)\n",
    "            result_df = result_df.reset_index()\n",
    "            \n",
    "            # print ('printing result_df')\n",
    "            # print (result_df.head())\n",
    "            # print ('printing df2_2')\n",
    "            # print (df2_2.head())\n",
    "            # debugging\n",
    "            # print (result_df)\n",
    "            # debugging\n",
    "\n",
    "            # Filtering rows with String for each sensor\n",
    "            SensorStringResult = df2_2[df2_2['DATETIME'].isin(result_df.loc[:,\"DATETIME\"].values.tolist())][Sensor]\n",
    "\n",
    "            # Adding Strings to Sensor in Sensor Dictionary\n",
    "            # I think that when due to a bug the dictionary become too big it stops growing...\n",
    "            # SensorDictionary[Sensor] = list(set(SensorStringResult))\n",
    "            StringListForCurrentSensor = list(set(SensorStringResult))\n",
    "            \n",
    "            # Debugging\n",
    "            # print (list(set(SensorStringResult)))\n",
    "            # Debugging\n",
    "            \n",
    "            # print ('Sensor: ' + Sensor + ' Processed')\n",
    "\n",
    "            # Adding String for Sensor to General String List\n",
    "            AllSensorsStringList.extend(StringListForCurrentSensor)\n",
    "\n",
    "            # Removing Duplicate Strings\n",
    "            AllSensorsStringList = list(set(AllSensorsStringList))\n",
    "            \n",
    "            print ('These are all the strings found in Sensor ' + Sensor + ': ' + str(StringListForCurrentSensor))\n",
    "    return (AllSensorsStringList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SetIndex(df2):\n",
    "        # Code to do Transposing\n",
    "    # Create two dataframes df1 only with tags and descriptions. df2 tag with values\n",
    "    df2_1 = df2[0:1]  # FIRST ROW\n",
    "    df2_2 = df2[1:len(df2)]  # SECOND TO LAST ROW\n",
    "    \n",
    "    # Indexing dataframe df1\n",
    "    df2_1 = df2_1.set_index('DATETIME')\n",
    "\n",
    "    # Indexing dataframe df2\n",
    "    df2_2 = df2_2.set_index('DATETIME')\n",
    "    return(df2_1, df2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadCSV(FileAndPath, file):\n",
    "    print ('Loading ' + file)\n",
    "    df2 = pd.read_csv(FileAndPath, low_memory=False)\n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FormatToPrevise(df2_1, df2_2):\n",
    "    # Dropping DATETime index to merge df1 and df2\n",
    "    df2_1 = df2_1.reset_index(drop=False)\n",
    "    df2_2 = df2_2.reset_index(drop=False)\n",
    "\n",
    "    # Converting Historian files to VTQ format (DATETime, TAGNAME, DESCRIPTION, VALUE)\n",
    "    mdf = pd.merge(pd.melt(df2_1, id_vars=['DATETIME'], var_name='TAGNAME',\n",
    "                           value_name='DESCRIPTION')[['TAGNAME', 'DESCRIPTION']],\n",
    "                   pd.melt(df2_2, id_vars=['DATETIME'], var_name='TAGNAME',\n",
    "                           value_name='VALUE'),\n",
    "                   on=['TAGNAME'])\n",
    "\n",
    "    # Sort columns by VTQ format\n",
    "    mdf = mdf[['DATETIME', 'TAGNAME', 'DESCRIPTION', 'VALUE']]\n",
    "    \n",
    "    return (mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SplitPreviseFormatCSVFile(mdf):\n",
    "    # Exporting PHD Tag CSV file\n",
    "    i = 0\n",
    "    rows = 1000000\n",
    "    totalRows = len(mdf)\n",
    "    loops = math.ceil(totalRows/rows) + 1\n",
    "\n",
    "    if totalRows > 1000000:\n",
    "        for j in range(loops): #need to round this\n",
    "            j = j + 1\n",
    "            print (str(j))\n",
    "            a = (rows*j) - rows\n",
    "            if totalRows <= rows:\n",
    "                b = totalRows\n",
    "                print('Exporting ' + str(CSVFileList[i].replace('.csv', '')) + ' Historian File')\n",
    "                print(\"\")\n",
    "                mdf[a:b].to_csv(FinalPath + '/' + str(CSVFileList[i].replace('.csv', '')) + '_Formatted.csv', index=False)\n",
    "            else:\n",
    "                if (rows*j) >= totalRows:\n",
    "                    b = totalRows\n",
    "                    print('Exporting ' + str(CSVFileList[i].replace('.csv', '')) + ' chunk' + str(j) + ' Historian File')\n",
    "                    print(\"\")\n",
    "                    mdf[a:b].to_csv(FinalPath + '/' + str(CSVFileList[i].replace('.csv', '')) + '_Formatted_chunk' + str(j) + '.csv', index=False)\n",
    "                else:\n",
    "                    b = (rows*j) - 1\n",
    "                    print('Exporting ' + str(CSVFileList[i].replace('.csv', '')) + ' chunk' + str(j) + ' Historian File')\n",
    "                    print(\"\")\n",
    "                    mdf[a:b].to_csv(FinalPath + '/' + str(CSVFileList[i].replace('.csv', '')) + '_Formatted_chunk' + str(j) + '.csv', index=False)\n",
    "    else:\n",
    "        mdf.to_csv(FinalPath + '/' + str(CSVFileList[0].replace('.csv', '')) + '_Formatted' + '.csv', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Main Code to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVFile = CSVFileList[0]\n",
    "CSVFileWithPath = CSVPath1 + \"/\" + CSVFileList[0]\n",
    "\n",
    "csvFileSizeGB = GetFileSize(CSVFileWithPath)\n",
    "\n",
    "if csvFileSizeGB > 1:\n",
    "    StringListForAllSensors = SplitCSVFile_GetStrings(CSVFileWithPath, CSVFile)\n",
    "    print ('These are all the strings found in the file: ' + str(StringListForAllSensors))\n",
    "else:\n",
    "    \n",
    "    df2 = LoadCSV(CSVFileWithPath, CSVFile)\n",
    "    \n",
    "    # Rename date and time column\n",
    "    print ('Renaming date and time column')\n",
    "    RenameColumn(df2)\n",
    "    # Apply Date and time format to dataframe\n",
    "    print ('Applying date and time format')\n",
    "    ApplyDateFormat(df2)\n",
    "    \n",
    "    # Apply Index and create two dataframes\n",
    "    df2_1, df2_2 = SetIndex(df2)\n",
    "    \n",
    "    print ('Extracting strings from csv file')\n",
    "    StringListForAllSensors = ExtractStrings(df2_1, df2_2)\n",
    "    \n",
    "    print ('These are all the strings found in the csv file: ' + str(StringListForAllSensors))\n",
    "\n",
    "# Convert all strings to a Dictionary\n",
    "StringListDict = {}.fromkeys(StringListForAllSensors, 'null')\n",
    "\n",
    "print (\"# Run following line to see the list of Strings\")\n",
    "## Copy the result and paste it in the following line of code\")\n",
    "StringListDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we can replace string with null or a number\n",
    "## I have assigned null to all strings by default all strings are replaced with null. If the strings need to be replaced with a number please replace the null with your desired number\n",
    "###Example\n",
    "###{'String 1': 'null',\n",
    "    'String 2': 1,\n",
    "    'String 3': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The replace code is still Work in Progress for files bigger than 1 GB\n",
    "#### What to do\n",
    "##### 1. Since the code does not load the whole CSV but splits it. The code needs to open each splitted file and do the replace\n",
    "##### 2. Once text is replaced the CSV file will be updated and load the next\n",
    "##### 3. Once all csv file are processed We need to convert them to Previse Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing known test to null\n",
    "df2_2.replace(\n",
    "{'Bad': 0,\n",
    " 'CLOSE': 0,\n",
    " 'Comm Fail': 'null',\n",
    " 'Configure': 'null',\n",
    " 'Error': 'null',\n",
    " \"Exception of type 'System.OutOfMemoryException' was thrown.\": 'null',\n",
    " 'Failed': 'null',\n",
    " 'I/O Timeout': 'null',\n",
    " 'Intf Shut': 'null',\n",
    " 'MEM_ERR': 'null',\n",
    " 'ON': 1,\n",
    " 'OPEN': 1,\n",
    " 'Off': 0,\n",
    " 'RUN': 1,\n",
    " 'STOP': 0,\n",
    " 'Scan Off': 'null',\n",
    " 'Scan Timeout': 'null'}\n",
    ", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we convert the data to linear format Previse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------\n",
    "## The format and Split will be done in the same step (this is to be done for files bigger than 1GB)\n",
    "### The code will loop thru each chunk and will Format to Previse then will Split the file\n",
    "### Each file will be a sensor\n",
    "### If the sensor csv file has more than 1MM rows the code will split that file also\n",
    "## -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FormatToPrevise code is still Work in Progress for files bigger than 1 GB\n",
    "### What to do\n",
    "#### 1. Since the code does not load the whole CSV but splits it. The code needs to open each splitted file and do the Formatting\n",
    "#### 2. The code will loop thru each Splitted CSV file and format each one of them\n",
    "#### 3. Once formatted the code will split the file into 1MM rows per CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdf = FormatToPrevise(df2_1,df2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run following line to see the TOP 25 rows for the new Previse Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdf.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Here we split the CSV data by file and each file will contain 1 million rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SplitPreviseFormatCSVFile code is still Work in Progress for files bigger than 1 GB\n",
    "### What to do\n",
    "\n",
    "### Improvement\n",
    "#### 1. I will update the code to create a CSV file per Sensor and if the sensor has more than 1MM rows it will splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SplitPreviseFormatCSVFile(mdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
